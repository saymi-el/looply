import OpenAI from 'openai';
import { env } from '../../../config/env.js';
import logger from '../../../config/logger.js';
import { PROMPTS, GENERATION_CONFIG } from '../shared/prompts.js';

// Types pour la réponse du script
export interface GeneratedScript {
    text: string;
    duration: number;
    videoPrompts: VideoPrompt[];
}

export interface VideoPrompt {
    positive: string;
    negative: string;
    scene: string;
    timing: {
        start: number;
        end: number;
    };
}

export interface ScriptGenerationInput {
    topic?: string;
    tone?: string;
    context?: any;
    duration?: number;
    visualStyle?: string;
    useModularGeneration?: boolean;
}

// Initialisation du client OpenAI
const openai = env.OPENAI_API_KEY ? new OpenAI({
    apiKey: env.OPENAI_API_KEY,
}) : null;

/**
 * Point d'entrée principal pour la génération de script
 */
export async function generateScript(input: ScriptGenerationInput): Promise<GeneratedScript> {
    const config = prepareGenerationConfig(input);
    
    logger.info({ 
        topic: config.topic,
        tone: config.tone,
        duration: config.duration,
        visualStyle: config.visualStyle,
        useModularGeneration: config.useModularGeneration
    }, 'Starting script generation');

    // Fallback si pas d'API key OpenAI
    if (!openai || !env.OPENAI_API_KEY) {
        logger.warn('OpenAI API key not configured, using fallback script generation');
        return generateFallbackScript(config);
    }

    try {
        if (config.useModularGeneration) {
            return await generateModularScript(config);
        } else {
            return await generateCombinedScript(config);
        }
    } catch (error) {
        logger.error({ error, config }, 'Failed to generate script with OpenAI');
        return generateFallbackScript(config);
    }
}

/**
 * Génération modulaire : script puis prompts vidéo séparément
 */
async function generateModularScript(config: GenerationConfig): Promise<GeneratedScript> {
    logger.info(config, 'Using modular generation approach');
    
    // Étape 1: Générer le script textuel
    const scriptText = await generateScriptText(config);
    
    // Étape 2: Générer les prompts vidéo à partir du script
    const videoPrompts = await generateVideoPrompts(scriptText, config);
    
    return {
        text: scriptText,
        duration: config.duration,
        videoPrompts
    };
}

/**
 * Génération combinée : script et prompts en une seule requête
 */
async function generateCombinedScript(config: GenerationConfig): Promise<GeneratedScript> {
    logger.info(config, 'Using combined generation approach');
    
    const prompt = buildCombinedPrompt(config);
    
    logger.info({ 
        promptPreview: prompt.substring(0, 200) + '...'
    }, 'Sending combined prompt to OpenAI');
    
    const response = await openai!.chat.completions.create({
        model: GENERATION_CONFIG.DEFAULT_MODEL,
        messages: [{ role: "user", content: prompt }],
        max_tokens: GENERATION_CONFIG.DEFAULT_MAX_TOKENS,
        temperature: GENERATION_CONFIG.DEFAULT_TEMPERATURE,
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
        throw new Error('No content generated by OpenAI');
    }

    logger.info({ 
        model: GENERATION_CONFIG.DEFAULT_MODEL,
        usage: response.usage,
        responseLength: content.length
    }, 'Received combined response from OpenAI');

    return parseCombinedResponse(content, config.duration);
}

/**
 * Génère uniquement le texte du script
 */
async function generateScriptText(config: GenerationConfig): Promise<string> {
    const prompt = buildScriptPrompt(config);
    
    logger.info({ 
        promptPreview: prompt.substring(0, 200) + '...'
    }, 'Sending script prompt to OpenAI');
    
    const response = await openai!.chat.completions.create({
        model: GENERATION_CONFIG.DEFAULT_MODEL,
        messages: [{ role: "user", content: prompt }],
        max_tokens: Math.ceil(GENERATION_CONFIG.DEFAULT_MAX_TOKENS * 0.6), // Moins de tokens pour le script seul
        temperature: GENERATION_CONFIG.DEFAULT_TEMPERATURE,
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
        throw new Error('No script content generated by OpenAI');
    }

    logger.info({ 
        scriptLength: content.length,
        usage: response.usage
    }, 'Generated script text successfully');

    return content.trim();
}

/**
 * Génère les prompts vidéo à partir d'un script existant
 */
async function generateVideoPrompts(scriptText: string, config: GenerationConfig): Promise<VideoPrompt[]> {
    const prompt = buildVideoPromptsPrompt(scriptText, config);
    
    logger.info({ 
        scriptPreview: scriptText.substring(0, 100) + '...',
        promptPreview: prompt.substring(0, 200) + '...'
    }, 'Sending video prompts generation to OpenAI');
    
    const response = await openai!.chat.completions.create({
        model: GENERATION_CONFIG.DEFAULT_MODEL,
        messages: [{ role: "user", content: prompt }],
        max_tokens: Math.ceil(GENERATION_CONFIG.DEFAULT_MAX_TOKENS * 0.8), // Plus de tokens pour les prompts détaillés
        temperature: GENERATION_CONFIG.DEFAULT_TEMPERATURE,
    });

    const content = response.choices[0]?.message?.content;
    if (!content) {
        throw new Error('No video prompts generated by OpenAI');
    }

    logger.info({ 
        usage: response.usage,
        responseLength: content.length
    }, 'Generated video prompts successfully');

    return parseVideoPromptsResponse(content, config);
}

/**
 * Construction du prompt pour génération de script seul
 */
function buildScriptPrompt(config: GenerationConfig): string {
    return replacePromptVariables(PROMPTS.SCRIPT_GENERATION, config);
}

/**
 * Construction du prompt pour génération de prompts vidéo
 */
function buildVideoPromptsPrompt(scriptText: string, config: GenerationConfig): string {
    const configWithScript = { ...config, script: scriptText };
    return replacePromptVariables(PROMPTS.VIDEO_PROMPTS_GENERATION, configWithScript);
}

/**
 * Construction du prompt combiné
 */
function buildCombinedPrompt(config: GenerationConfig): string {
    return replacePromptVariables(PROMPTS.COMBINED_GENERATION, config);
}

/**
 * Remplace les variables dans les templates de prompts
 */
function replacePromptVariables(template: string, config: GenerationConfig & { script?: string }): string {
    const segmentDuration = Math.floor(config.duration / GENERATION_CONFIG.SEGMENTS_COUNT);
    const segmentDuration2 = segmentDuration * 2;
    
    return template
        .replace(/{topic}/g, config.topic)
        .replace(/{tone}/g, config.tone)
        .replace(/{duration}/g, config.duration.toString())
        .replace(/{wordCount}/g, Math.round(config.duration * GENERATION_CONFIG.WORDS_PER_SECOND).toString())
        .replace(/{visualStyle}/g, config.visualStyle)
        .replace(/{segmentDuration}/g, segmentDuration.toString())
        .replace(/{segmentDuration2}/g, segmentDuration2.toString())
        .replace(/{script}/g, config.script || '');
}

/**
 * Parse la réponse combinée (script + prompts vidéo)
 */
function parseCombinedResponse(content: string, duration: number): GeneratedScript {
    try {
        logger.info({ originalContent: content.substring(0, 500) + '...' }, 'Parsing combined response');
        
        const cleanContent = cleanJsonResponse(content);
        const parsed = JSON.parse(cleanContent);
        
        logger.info({ parsedKeys: Object.keys(parsed) }, 'Parsed combined JSON successfully');
        
        if (parsed.text && parsed.videoPrompts && Array.isArray(parsed.videoPrompts)) {
            return {
                text: parsed.text,
                duration: duration,
                videoPrompts: normalizeVideoPrompts(parsed.videoPrompts, duration)
            };
        }
    } catch (error) {
        logger.warn({ error, content: content.substring(0, 200) + '...' }, 'Failed to parse combined JSON response');
    }

    // Fallback: traiter comme du texte simple
    return {
        text: content.trim(),
        duration: duration,
        videoPrompts: generateDefaultPrompts(duration)
    };
}

/**
 * Parse la réponse de génération de prompts vidéo
 */
function parseVideoPromptsResponse(content: string, config: GenerationConfig): VideoPrompt[] {
    try {
        logger.info({ content: content.substring(0, 300) + '...' }, 'Parsing video prompts response');
        
        const cleanContent = cleanJsonResponse(content);
        const parsed = JSON.parse(cleanContent);
        
        if (parsed.videoPrompts && Array.isArray(parsed.videoPrompts)) {
            logger.info({ promptsCount: parsed.videoPrompts.length }, 'Parsed video prompts successfully');
            return normalizeVideoPrompts(parsed.videoPrompts, config.duration);
        }
    } catch (error) {
        logger.warn({ error, content: content.substring(0, 200) + '...' }, 'Failed to parse video prompts JSON');
    }

    // Fallback: générer des prompts par défaut
    return generateDefaultPrompts(config.duration, config.topic);
}

/**
 * Nettoie la réponse JSON d'OpenAI
 */
function cleanJsonResponse(content: string): string {
    return content
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
}

/**
 * Normalise et valide les prompts vidéo
 */
function normalizeVideoPrompts(prompts: any[], duration: number): VideoPrompt[] {
    return prompts.map((prompt: any, index: number) => ({
        scene: prompt.scene || `Scène ${index + 1}`,
        positive: prompt.positive || '',
        negative: prompt.negative || GENERATION_CONFIG.DEFAULT_NEGATIVE_PROMPTS,
        timing: prompt.timing || {
            start: Math.floor(index * duration / GENERATION_CONFIG.SEGMENTS_COUNT),
            end: Math.floor((index + 1) * duration / GENERATION_CONFIG.SEGMENTS_COUNT)
        }
    }));
}

/**
 * Prépare la configuration de génération
 */
function prepareGenerationConfig(input: ScriptGenerationInput): GenerationConfig {
    return {
        topic: input.topic || 'contenu généraliste',
        tone: input.tone || 'professionnel',
        duration: input.duration || 15,
        visualStyle: input.visualStyle || GENERATION_CONFIG.VISUAL_STYLES.PROFESSIONAL,
        useModularGeneration: input.useModularGeneration ?? true
    };
}

/**
 * Génère un script de fallback
 */
function generateFallbackScript(config: GenerationConfig): GeneratedScript {
    const text = `Découvrez ${config.topic} sous un angle ${config.tone}. Cette approche innovante transforme notre compréhension du sujet. Grâce aux dernières avancées, nous pouvons maintenant explorer de nouvelles perspectives fascinantes qui ouvrent des horizons inattendus.`;
    
    return {
        text,
        duration: config.duration,
        videoPrompts: generateDefaultPrompts(config.duration, config.topic)
    };
}

/**
 * Génère des prompts par défaut
 */
function generateDefaultPrompts(duration: number, topic?: string): VideoPrompt[] {
    const segmentDuration = Math.floor(duration / GENERATION_CONFIG.SEGMENTS_COUNT);
    const basePrompt = topic ? `professional ${topic} content` : 'professional content';
    
    return [
        {
            scene: "Introduction",
            positive: `${basePrompt}, clean modern design, high quality, detailed, cinematic lighting`,
            negative: GENERATION_CONFIG.DEFAULT_NEGATIVE_PROMPTS,
            timing: { start: 0, end: segmentDuration }
        },
        {
            scene: "Développement",
            positive: `${basePrompt}, dynamic composition, vibrant colors, sharp focus, professional`,
            negative: GENERATION_CONFIG.DEFAULT_NEGATIVE_PROMPTS,
            timing: { start: segmentDuration, end: segmentDuration * 2 }
        },
        {
            scene: "Conclusion",
            positive: `${basePrompt}, inspiring finale, excellent quality, detailed, masterpiece`,
            negative: GENERATION_CONFIG.DEFAULT_NEGATIVE_PROMPTS,
            timing: { start: segmentDuration * 2, end: duration }
        }
    ];
}

// Types internes
interface GenerationConfig {
    topic: string;
    tone: string;
    duration: number;
    visualStyle: string;
    useModularGeneration: boolean;
}